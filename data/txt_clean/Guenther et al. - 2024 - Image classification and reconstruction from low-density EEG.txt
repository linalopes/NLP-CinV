OPEN

Image classification and reconstruction from low-density EEG

Sven Guenther  ͷ  , Nataliya Kosmyna  ͸  &amp; Pattie Maes ͸

Recent advances in visual decoding have enabled the classification and reconstruction of perceived images from the brain. However, previous approaches have predominantly relied on stationary, costly equipment like fMRI or high-density EEG, limiting the real-world availability and applicability of such projects. Additionally, several EEG-based paradigms have utilized artifactual, rather than stimulusrelated information yielding flawed classification and reconstruction results. Our goal was to reduce the cost of the decoding paradigm, while increasing its flexibility. Therefore, we investigated whether the classification of an image category and the reconstruction of the image itself is possible from the visually evoked brain activity measured by a portable, ;-channel EEG. To compensate for the low electrode count and to avoid flawed predictions, we designed a theory-guided EEG setup and created a new experiment to obtain a dataset from Ϳ subjects. We compared five contemporary classification models with our setup reaching an average accuracy of ͹ͺ.ͺ% for ͸Ͷ image classes on hold-out test recordings. For the reconstruction, the top-performing model was used as an EEG-encoder which was combined with a pretrained latent diffusion model via double-conditioning. After fine-tuning, we reconstructed images from the test set with a ͷͶͶͶ trial ͻͶ-class top-ͷ accuracy of ͹ͻ.͹%. While not reaching the same performance as MRI-based paradigms on unseen stimuli, our approach greatly improved the affordability and mobility of the visual decoding technology.

The past two decades have seen significant advances of classifying the object category of a perceived image and reconstructing visual stimuli from brain   recordings 1-4 . The motivation for this has been two-fold. On the one hand, researchers hope to derive new insights into how the brain processes visual   stimuli . On the other hand, 5 reconstructing visual information from someone's brain could offer an intuitive communication channel for patients suffering from   paralysis . However, while classification accuracies and reconstruction qualities have 6 steadily increased, two key challenges have remained largely unaddressed. First, previous studies have predominantly relied on costly and stationary equipment for functional Magnetic Resonance Imaging (fMRI), Magnetoencephalography (MEG) or high-channel Electroencephalography (EEG). While the monetary constraints limit the widespread use of these technologies, the immobility and long setup times of the equipment render a possible real-world application unfeasible. Second, most of the high-channel EEG studies have used a public dataset that presented all images of a class sequentially in   blocks . This allowed artifactual classifications based 7 on block-level temporal correlations in the EEG signal rather than stimulus-associated patterns generated by the brain, as pointed out by Li et al. . Thus, the models mostly learned to distinguish images not from the evoked 8 signal, but from the temporal dynamics of the measurement tool.

Portable EEG devices may increase mobility and could greatly reduce the cost and preparation time with a reduction in electrode numbers. Arguably, a lower channel count corresponds to less information available for the classification. However, this effect may be mitigated by focusing on the most predictive electrode locations detected by previous   research 9,10 . Additionally, the artifactual classifications of prior EEG studies could be avoided by randomly shuffling the stimuli after every experiment run during the data acquisition. Therefore, we propose to address the key challenges in the following three ways:

- 1. Reducing costs: To loosen financial constraints and enable more researchers to work with our findings, we employ a low-cost and commercially available EEG system.
- 2. Increasing flexibility: To avoid stationary devices or equipment with prolonged setup times, we utilize a portable, 8-channel EEG with a preparation time of less than 15 minutes. Furthermore, we use previous sci-

ͷ School of Computation, Information and Technology, Technical University of Munich, Munich, Germany.  Media ͸ Lab, Massachusetts Institute of Technology, Cambridge, USA.  email: sven.guenther@tum.de

Scientific Reports

|        (2024) 14:16436

glyph&lt;c=25,font=/MGEUUU+Corbel&gt;

o.:ȋͬͭͮ

Vol:.(1234567890)

entific findings to arrange the electrode positions in a theory-guided fashion for optimal use of the reduced channels. Thereby, we want to pave the way to the real-world usability of such systems.

- 3. Creating a new Image-EEG dataset: We run a new experiment with 9 subjects recording the EEG activity evoked by natural image stimuli to evaluate classification and reconstruction performances. We ensure the classification based on block-level temporal correlations is impossible by randomly shuffling the order of the presented images for every recording session.

Our study aims to implement the solutions mentioned above and to assess the classification and reconstruction performances using the new setup. Therefore, we first design the experimental paradigm to collect EEG-image pairs from 9 subjects. Subsequently, we adapt five current state-of-the-art EEG classification models to work with the streamlined recording system. We compare the different models to explore the boundaries of image object classification from EEG with our setup given the new dataset. Additionally, the classification accuracy is used as a validity measure to check how well our setup may discern the visually evoked potentials (VEP) of different image classes. Lastly, we attempt the reconstruction of the visual input stimuli from the EEG data. To achieve this, we modify the top-performing classifier to work as an EEG-encoder used to double-condition a pre-trained latent diffusion model (LDM) via additional projectors. We jointly finetune the EEG encoder, the projectors, and the cross-attention heads of the stable diffusion model, similar to Chen et al.  who used embeddings extracted from 1 fMRI recordings. Finally, we aim to answer whether the classification of an image category and reconstruction of the image itself is feasible from the visually evoked brain activity measured with a portable, low-density EEG.

Methods Experiment

Subjects

Nine healthy participants (5 female), aged 20-33 (M: 22.5, SD: 1.8) were recruited at the research laboratory at Massachusetts Institute of Technology (MIT). All subjects were either students, research assistants, or research scientists at MIT. One subject was excluded for missing several recording sessions. All had normal or correctedto-normal vision. Subjects received $100 compensation in two gift cards: $50 for participation and an additional $50 as a bonus, earned by correctly answering questions about presented images to increase motivation during the experiment. Informed consent from all subjects - for both study participation and publication of images during the study in an online open-access publication was obtained. The experimental protocol was approved by the Institutional Review Board of MIT.

EEG setup

To record brain signals, we used the portable 8-channel g.tec Unicorn Hybrid Black EEG sampling at 250 Hz with an integrated amplifier. The electrodes of the Unicorn allowed for both dry and wet (gel-based) recordings. According to the most predictive channel locations identified in prior   research 9,10 , we adapted the default channel positions of the Unicorn to allow an electrode positioning corresponding to the PO8, O2, O1, PO7, PO3, POZ, PO4, and Pz locations defined by the 10-20 international   system 11 , respectively (Fig. 1A). The reference electrodes were placed on the left and right mastoids. The designed setup is displayed in Fig. 1B. We used the Unicorn Suite Hybrid Black software to ensure good signal quality and to visually inspect the signal. To collect the data stream from the EEG headset, we employed the Lab Streaming Layer (LSL) 12 .

Figure 1.    A ( ) 10-20 international system with the employed electrode locations highlighted in yellow ( B ) g.tec Unicorn Hybrid Black system configuration mounted onto an artificial head.

Image selection

In our experiment, we presented 600 images, belonging to 20 image classes (30 per class) to the subjects during a session. 19 classes were a subset selected from the ImageNet Large Scale Visual Recognition   Challenge 13 and 30 human face images were added from a Kaggle   dataset 14 to create an additional Face category. This additional image category aimed to leverage the distinctive response human faces evoke in the visual   cortex 9,15 . We ensured that the individual images per class only showed the respective object category and contained coherent low-level characteristics, like distinct silhouette and luminance properties, which have been shown to improve differentiation of   classes . The low-level coherence may be observed from the image class means shown in Supplementary 9 Fig. 1. To facilitate the selection, we chose one prototypical image per class and selected 29 out of the 50 most similar images for the dataset. For details on the similarity score, see the Supplementary Materials.

Experiment setup

During the experiment, the 600 images were sequentially displayed for 2s each, separated by a 1s uniform gray screen to flush the visual percept of the preceding image. The way the brain encodes varying degrees of category abstraction unfolds sequentially over   time 16 . Low-level information about displayed objects, like their shape, may be decoded as soon as 60ms post-presentation 17 , whereas high-level features, such as the category of the object, seem to be extractable only after   100ms 18 . Prior studies have used image exposure durations between   50ms 19 and   2s 20 . We opted for the longer duration of 2s to reduce the risk of missing class discriminative information. Consequently, for the entire set of 600 stimuli, the session duration extended to 30 minutes. We selected the exposure time and number of images to strike a balance with regards to the session duration, ensuring that the experiment remained sufficiently brief for participants to sustain their attention. Analogous to the most frequently used   dataset  for visual decoding, we decided to keep the number of samples per class higher than the 7 number of classes. However, in contrast to this dataset, we prevented classifications based on block-level temporal correlations by randomly shuffling the image presentation order for each session. To run the experiment, we used the Psychopy   software 21 and utilized the LSL LabRecorder to combine and synchronize the experiment and EEG streams. All processing steps and analyses were conducted using Python.

The experiments were conducted in a darkened experiment room with no windows at the MIT Media Lab to minimize external visual distractions while subjects focused on the screen. After finishing the EEG setup, the participant took a comfortable seating position with the 15.6' experiment screen on a desk in front of them at a distance of roughly 0,5 m. Subjects were instructed to sit still and the screen was adjusted such that the top matched the eye level of the participant. After the setup, the experimenter left the room upon which the subject could start the experiment by clicking the space bar on a keyboard in front of them by themselves. The experimenter was not present during the recordings. Usually, between one to three sessions were recorded in a row, depending on how attentive the subject felt (self-reporting). In between sessions, we checked the signal quality using the Unicorn Recorder. Each subject underwent a total of 12 sessions of wet recordings. For the first subject, we additionally analyzed 12 dry recordings to evaluate the necessity of using conductive gel in the experiment. Fig. 2 displays the experiment process.

Preprocessing

After collecting the EEG data from each subject, we applied a preprocessing pipeline to increase the signal-tonoise ratio (SNR) of the recordings and to create a data format suitable for the subsequent classification. The pipeline was applied to each recording session separately and consisted of the rejection of bad trials, filtering, channel-wise z-normalization, and clamping (&gt;20 std. dev.). A trial was regarded as the EEG activity accompanying an image from its onset until it disappeared. Since each image was presented for two seconds, we selected the 500 EEG samples after the image onset time to represent the trial. Therefore, each trial had a dimension of (8500), given the eight-channel setup of the EEG. The bad trial rejection was adapted from Bigdely-Shamlo et al. 22 and involved the detection of trials with NaN values, flat signals or either too low or high inter-channel correlations. Its output was a mask marking bad trials. For the filtering we used a separate highpass (1 Hz), lowpass (95 Hz) and notch filter (60 Hz) applied before the trial segmentation to avoid edge   artifacts 23 . Then, we segmented the data into trials again and used the previously calculated mask to exclude bad trials. Supplementary Table 1 shows the percentage of dropped trials, as well as the total number of trials per participant. For subjects 3, 4, and 7 one recording needed to be excluded due to a malfunctioning electrode. Lastly, we inspected the preprocessed data

Figure 2. Subject sitting in front of experiment laptop with images being presented for 2s each with 1s gray screen in between.

Vol.:(0123456789)

Vol:.(1234567890)

to visually verify that it contained typical VEPs (Supplementary Fig. 3). A detailed description of the bad trial rejection and filtering is given in the Supplementary Materials.

Classification

Following the preprocessing of the EEG data to ensure signal quality, we aimed to classify the image class of the perceived images from the associated brain activity. To this end, we compared five state-of-the-art EEG classification models: (1)   EEGNet 24 , (2)   TSCeption 25 , (3) EEG-ChannelNet 26 , (4) EEG   Conformer 27 , and (5) the EEG-toimage transformation approach by Mishra et al. 28 . For ease of reference, we will refer to all but the EEG-to-image classification model as 'deep classifiers' , as they use a neural network to arrive at the final classification. While the deep classifiers all utilize convolutional neural networks (CNN) for the classification, their architectures differ at fundamental levels. The EEGNet is a compact CNN that uses 2D convolutions over the time dimension, as well as depthwise convolutions along the spatial dimension to obtain frequency-specific spatial filters followed by a separable convolution   layer 29 to extract complex feature maps from the EEG signal. It has been validated on multiple EEG-based paradigms, including various VEP classification   studies 4,20 . In turn, TSCeption draws inspiration from GoogleNet's inception   block 30 , employing multiple temporal and spatial convolutional kernels, chosen based on sampling-rate ratios and channel-locations, respectively, for diverse feature learning. EEG-ChannelNet combines elements from EEGNet and TSCeption and was conceptualized to improve upon the EEGNet in visual classification. It contains multiple temporal and spatial filters, like TSCeption, but utilizes different dilated kernels to capture various temporal patterns. Most notably, the EEG-ChannelNet aims to extract complex spatio-temporal representations by adding a block of residual layers consisting of 2D convolutions after the initial temporal and spatial filtering. The EEG Conformer mainly differs from the other models by introducing self-attention modules after its CNN backbone to improve the detection of global patterns in the signal. Finally, the EEG-to-image-based model stands apart by transforming EEG recordings into grayscale images, leveraging a pre-trained image classification model for feature extraction and using a machine learning classifier as output.

Except for the EEGNet and the EEG-to-image classification algorithms, we obtained the original implementations and adapted them to work in our framework. For the EEGNet, we employed the implementation from Braindecode 31 . The EEG-to-image transformation strategy was constructed according to the description given by Mishra et al. 28 and Zhang et al. 32 . The subject-wise classification models were trained on all but two recordings per subject which served as hold-out validation and test set, respectively. The objective during training was to minimize the cross-entropy loss for which we used the Gradient Descent algorithm including Adaptive Movement Estimation (Adam) 33  and Weight Decay (L2 penalty) 34 . Additionally, we employed a one-cycle learning rate   scheduling 35 for faster convergence, limiting the number of training epochs to 30 for all deep classifiers, except for the EEG Conformer. Notably, we investigated different hyperparameter combinations for each model, as explained in the Supplementary Materials. All models were trained on a NVIDIA GeForce RTX 3070 GPU.

For each classification model and subject, we retrained the hyperparameter configuration yielding the best validation accuracy on the training and validation set. The resulting model was used once on the test set to obtain the final test result. Notably, in the data split, we selected the recording with the fewest missing trials as the test recording to accurately estimate accuracies for each image class. To estimate the selection effect, we conducted a k-fold cross-validation run for every subject, where k equaled the number of non-test recordings. During this approach, the best-performing model was iteratively fitted on k-1 recordings and evaluated on the left-out recording before averaging across the accuracies. We then used a Wilcoxon Signed-Rank Test to examine whether the accuracy on the selected test recordings was statistically significantly higher than the average accuracy on the cross-validation runs. Additionally, we were looking for the best classification algorithm which was supposed to be adapted to work as an EEG encoder for the reconstruction task. Therefore, we examined whether the best model obtained a significantly higher accuracy than the other models with a Wilcoxon Signed-Rank test. We corrected for multiple testing using Bonferroni, yielding a Type 1 error probability of α = 0.01 . We opted for the non-parametric test for its robustness against violations of the normality assumption, which was especially relevant given the small sample size of n = 8.

Reconstruction

Latent diffusion model

To generate images, we employ a latent diffusion model (LDM) 36 . A Diffusion Model (DM) 37 is a probabilistic model consisting of a forward and backward process. For image generation, the forward process incrementally adds noise to an image over a series of steps until the input is turned into Gaussian noise. In the backward process, the model aims to gradually remove the noise in a step-wise fashion until a sample in the original input data distribution is received. The denoising can be described as the reverse process of a Markov Chain of T steps, where the states, t = 1, ..., T , represent progressive additions of noise in the forward process. At each step in the backward process, a denoising function, /epsilon1 θ ( xt , t ) , takes in a noisy version, xt , of the input, x , and predicts a denoised version, x t ( -1 ) . The denoising function is commonly realized as a   UNet 38 . Because operating in pixel space comes with a high memory demand, Rombach et al. 36 have suggested to feed the input image through a Vector Quantized-Variational Autoencoder (VQ-VAE) 39 to obtain a lower dimensional representation. Namely, the encoder, E , of the VQ-V AE is used to reduce the dimensionality of the high-dimensional image, x ∈ R HxWx 3 , to its latent representation, z = E ( x ) . Subsequently, z is passed to the DM and its (latent) output is decoded back to the image space by the VQ-VAE's decoder, D .

Additionally, Rombach et al. 36 have introduced a conditioning mechanism into the diffusion process to control the generation of images from other inputs, y , like text. This is realized by employing a domain-specific projector, τθ , such that τθ ( y ) ∈ R Mxd τ can be linked to cross attention layers in the modified UNet, /epsilon1 θ ( zt , t , τθ ( y )) . M is an adjustable parameter. The attention is modeled as

<!-- formula-not-decoded -->

where Q = W i ( ) Q · ϕ ( i zt ) , K = W i ( ) K · τθ ( y ) , V = W i ( ) V · τθ ( y ) and W i ( ) Q ∈ R dxd τ , W i ( ) K ∈ R dxd τ , W i ( ) V ∈ R dxd i /epsilon1 are trainable projection matrices. Q, K, and V are also known as query, key, and value, respectively. ϕ ( i zt ) marks an intermediate layer of /epsilon1 θ . For an x y -input pair, the conditional LDM can be trained with the objective

<!-- formula-not-decoded -->

Reconstructing images from the brain via double-conditioned LDM

For the reconstruction of the perceived images from the evoked brain signals, we adapted the framework by Chen et al. 1 , which utilized fMRI representations to double-condition a pretrained LDM. In our case, the domainspecific projector, τθ , takes an EEG embedding as input and feeds to the cross-attention heads in the UNet. Additionally, another projector, σθ , is used to obtain σθ (τθ ( y )) ∈ R 1 xdt , matching the time embedding dimension, dt . Thereby, the EEG embedding may be added to the time step embeddings in the UNet for additional time steps onditioning c 40 . Thus, the optimization objective becomes

<!-- formula-not-decoded -->

EEG encoder

To obtain an EEG encoder, we modified the best-performing model by exchanging its classifier head with two linear layers. The first layer contained 512 nodes and was used as an embedding, while the second layer was utilized as the classifier output. To ensure this adapted model maintained comparable classification accuracy, we repeated the hyperparameter optimization with the same data split as used for the prior models and evaluated the model performance on the test set for each subject. The data and model of the subject with the best accuracy were then selected for reconstruction. We employed the best performing model anticipating it would most effectively extract class discriminative information, while expecting its efficiency to be a limiting factor to the reconstruction performance. We removed the final classification layer and used the rest of the model as the EEG encoder. Whereas Chen et al.  employed an embedding dimension of 1024 which was halved in the linear 1 projector to match the dimensions of the LDM, we reduced the computational complexity by directly mapping to a dimension of 512 in the EEG encoder and using a 1x1 convolution projector to match the expected depth. Fig. 3 presents the different stages employed for the reconstruction, from learning the EEG encoder to conditioning the pretrained LDM for the image generation.

Finetuning

The LDM was pre-trained in a separate conditioning context, therefore, we had to finetune it to use the encoded EEG signals to condition the generative model. To fine-tune the pre-trained LDM, we employed the EEG-image pairs of the training and validation set of the subject for training and validation, respectively. However, we only updated the EEG encoder, as well as the cross-attention and projection heads, similar to Chen et al. . Thus, for 1

Figure 3. In the classification task, we aimed to predict the image class from VEPs. The best performing classifier, C, was adapted to work as an EEG encoder which we used to condition the pretrained LDM with the resulting EEG embedding, y . Therefore, we employed two projectors, τθ and σθ , which respectively matched the dimensions expected by the cross-attention heads and the time embeddings in the UNet, /epsilon1 θ , for cross-attention and time steps conditioning. The iteratively (de)noised images shown in the reconstruction were only used for illustrative purposes, as the actual LDM works in the latent image space.

Vol.:(0123456789)

Vol:.(1234567890)

a given EEG-image pair, the VQ-VAE encoder turned the image into the latent space representation, which was then employed as the objective during training. The associated EEG signal was transformed into an embedding and passed to the cross-attention modules via the convolution projector, where it functioned as the key and value. Additionally, the projected embedding was added to the time embedding in the UNet for time-step conditioning. For the training, we adopted the procedure and hyperparameters from Chen et al. , but employed a smaller 1 learning rate of 5e-6 and only fine-tuned for 200 epochs. Similar to the classification task, the final fine-tuned model was trained on the train and validation set and evaluated on the test recording. The finetuning was able to run on a NVIDIA GeForce RTX 3070 GPU.

Image generation and evaluation

Eventually, the fine-tuned model could be used to reconstruct perceived images from associated EEG signals. The images were generated in a 256x256 pixel format using 250 sampling steps with the Pseudo Linear MultiStep (PLMS)   method 41 . For each EEG-image pair in the test set, we generated five reconstructions with different random states and reported the 1000 trial 50-class top-1 accuracy on the best-generated sample per image, as in Chen et al. . Additionally, we calculated the mean 50-class top-1 accuracy across the five samples per image 1 to evaluate the generation consistency. The utilized image classifier in the metric was a vision   transformer 42 .

However, as the image classes in the test set were the same used in the training and validation sets, this approach did not allow a direct comparison to the reconstructions done by Chen et al. . Therefore, we obtained 1 an additional recording of the best-performing subject but presented 300 images from 10 object categories (30 images per class) that were not used during training. This yielded another test recording with unseen image classes to test the model's ability of zero-shot learning. To clarify which test set we refer to in the remainder of this study, we will call the test set that shared image classes with the training and validation set the 'base test set' . The other test recording will be coined in this paper as the 'advanced test set' . Similar to the base test set, we prepared the advanced test set with the preprocessing pipeline. We report the results on both test sets to estimate the capability of our approach to reconstruct images from known, as well as unknown image categories.

Additionally, we further aimed to investigate the effect of the classification model on the reconstruction outcome. First, we tested our expectation that the performance of the classification model which we used for the EEG encoder would limit the reconstruction performance. Therefore, we computed the point-biserial correlation between the classifier performance per image (0 for misclassification; 1 for correct classification) and the reconstruction performance as evidenced by the 1000 trial 50-class top-1 accuracy on the best-generated sample per image. Second, we examined whether the reconstruction depended on the actual EEG input, instead of being driven solely by the model. For this we used the inter-trial EEG recordings, which contained no stimulusassociated signal, as input for the reconstruction. Since the inter-trial durations were only 1s long, we upsampled them to obtain the same length as for the original input and applied the channel-wise normalization.

The code to replicate the data acquisition, preprocessing, classification, and reconstruction conducted in this study is openly accessible via the following link: https://  github.  com/  mitme  dialab/  eegre  const  ructi  on.

Institutional review board

The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board of MIT (protocol code 2107000428A003, 09/15/2021).

Results Classification

With a test accuracy of 34.4% averaged across subjects, the EEGNet model significantly outperformed all other classifiers ( p &lt; .01 ). It was followed by the TSception and the EEG Conformer with a mean performance of 22.6% and 21.2%, respectively. In contrast, the EEG ChannelNet only reached 15.6% accuracy, while the EEGto-Image paradigm ranged just around chance level with 6.4%. The comparison is shown in Fig. 4. To estimate the effect of our test set selection bias, we compared the mean average cross-validation result on the training and validation sets of each subject with the mean test set accuracy. Using the EEGNet, the average accuracy on the test sets (34.4%) was higher than that of the cross-validation (31.0%), however, the difference was not statistically significant ( p = 0.054 ).

Table 1 shows the best test accuracies for each model and subject.The performance differed strongly across participants with a standard deviation of 15.1% for the EEGNet. Notably, the EEGNet was not only the best performing model but also the most consistent one regarding the hyperparameter search. Namely, the most complex architecture possible in the search with 64 temporal filters, max pooling and a Dropout probability of 0.25 yielded the superior result across subjects. Furthermore, we compared the classification performance with the EEGNet on the wet and dry recordings of Subject 1. Whereas the test accuracy on the wet recordings reached 52.5%, the same model only achieved 24.2% on the dry measurements.

Not surprisingly, the Face category obtained the highest accuracy (55.42%) of all image classes, as shown in Table 2. Across subjects, the Face class was followed by the Red Wine and Airliner image category. The least successfully predicted class was the Pretzel category with 20.86% accuracy. Additionally, we computed the confusion matrix for the best performing subject/model combination (Subject 1, EEGNet), presented in Fig. 5, to investigate common misclassifications. For subject 1, the image classes containing facial features ( Face and Jack-o-Lantern ), as well as the animal categories ( Panda and Tiger ) obtained the highest accuracies. In turn, the Castle class seemed to be hardest to classify. Common misclassifications occurred between semantically similar image categories, like the Dog class which was frequently misinterpreted as Panda or Tiger .

Table 1. Classification accuracy in percentage on hold-out test recording for each model and subject, respectively. Best result of each subject is highlighted in bold. The last column shows mean accuracy and standard deviation for each model. With the best performance per subject highlighted in bold.

|            |   P001 |   P002 |   P003 |   P004 |   P005 |   P006 |   P007 |   P008 | Mean (SD)   |
|------------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
| EEGNet     |   52.5 |   40.2 |   15.5 |   51.2 |   11.5 |   29.3 |   48.7 |   25.9 | 34.4 (15.1) |
| TSCeption  |   43.3 |   25.5 |   12   |   35.3 |    8   |   19.7 |   27.2 |   10.1 | 22.6(11.8)  |
| Conformer  |   41.3 |   19.3 |   11   |   33.7 |    8.8 |   20.2 |   29.8 |   14.3 | 22.3 (10.8) |
| ChannelNet |   37.9 |   12.1 |    7.3 |   27.8 |    5.8 |    9.3 |   19.8 |    5   | 15.6(11.4)  |
| EEG-to-Img |    5   |    5.7 |    5.7 |    9.2 |    6   |    6.7 |    8.8 |    4.5 | 6.4(1.6)    |

Table 2. Average classification accuracy (%) and standard deviation across subjects with EEGNet model for each image class.

| Image class    | Mean accuracy   |
|----------------|-----------------|
| Airliner       | 46.88 (29.09)   |
| Fish           | 29.17(18.32)    |
| Banana         | 23.75 (19.80)   |
| Basketball     | 29.58 (17.31)   |
| Broccoli       | 37.50 (23.08)   |
| Castle         | 29.35 (17.61)   |
| Daisy          | 27.85 (11.19)   |
| Dog            | 28.33 (17.00)   |
| Face           | 55.42 (28.95)   |
| Jack-o-Lantern | 35.00 (24.49)   |
| Orange         | 28.33 (15.33)   |
| Panda          | 29.17(22.24)    |
| Pizza          | 46.67 (23.44)   |
| Pretzel        | 20.86(12.52)    |
| Red Wine       | 50.00 (23.23)   |
| School Bus     | 41.25 (23.63)   |
| Soccer Ball    | 22.14(17.05)    |
| Strawberry     | 31.67 (15.53)   |
| Tennis Ball    | 27.50(16.88)    |
| Tiger          | 45.42 (21.52)   |

Vol.:(0123456789)

Vol:.(1234567890)

Figure 5. Confusion matrix displaying true and predicted labels for each image class for the EEGNet model on the test recording from Subject 1. Ground truth labels are presented on the y-axis and predicted labels on the x-axis.

Reconstruction

For the reconstruction task, we modified the best-performing model (EEGNet) to obtain the EEG encoder. For simplicity, we called the new model EEGNet+. To verify that EEGNet+ retained a similar performance as the initial EEGNet, we evaluated it on the base test set for each subject. With an average test accuracy of 34.6%, its performance did not differ significantly from EEGNet ( p = 0.61 ). After pretraining the EEGNet+ model in the classification task, we discarded the output linear layer to obtain our EEG encoder and combined it with the pre-trained LDM. Upon fine-tuning the EEG encoder, as well as the attention and projection heads, we tried to reconstruct the images from their associated EEG signals. We will now report the results on Subject 1, who had the best performance in the classification task. The reconstruction results for an additional subject (subject 7) can be found in Supplementary Fig.  13.

On the base test set, the 1000 trial 50-class top-1 accuracy on the best-generated images, as well as the average 50-class top-1 accuracy across the five generated samples per image, were 35.3% and 30.9%, respectively. Fig. 6 displays a selection of good and bad reconstructions of our approach. Similar to the classification task, the reconstruction of the Face images was the best, even though facial details were often not represented correctly. Additionally, the reconstruction of the pictures showing an animal, especially for the Tiger and Panda classes, worked better than for most inanimate objects. Notably, in many cases during which the generated images differed from the ground truth, the shape, but not the color, seemed to be correct. Moreover, the incorrectly generated samples often depicted one of the other object categories in the dataset. In other cases, the falsely reconstructed image contained a mix of the ground truth and a different class, like a jack-o-lantern with a panda face. These phenomena can be observed in the reconstructions shown in Fig. 6B.

The advanced test set contained previously unseen image categories ( Blue Bird , Cat Clock , , Golf Ball , Horse , Pineapple , Police Truck , Shark , Ship , and Sunflower ). On this test set, we achieved a lower 1000 trial 50-class top-1 accuracy of 8.2% on the best-generated images and an average of 7.2% across the five generated samples per image. For the advanced test set, the generated images were highly variable and usually depicted the wrong image class which had a similarity to the image categories that were used to finetune the LDM. Fig. 7 shows a selection of reconstructions for each image category in the advanced test set. Notably, for some of the new image classes that exhibited similarity to the categories employed during training (e.g., Cat and Tiger ), the model consistently generated its corresponding class from the set of previously observed categories. While some of the reconstructions matched the shape of the ground truth image, this was not always the case, as can be seen for the Blue Bird or Shark image.

(B)

Figure 6. Selection of ( A ) good reconstructions and ( B ) typical failed reconstructions for an example of each image class from the base test set. Ground truth (GT) is presented on the left with the reconstructed image to the right for each column, respectively. The presented ground truth images, except for the human face image, were taken from the ImageNet Large Scale Visual Recognition   Challenge 13 dataset which permits usage for noncommercial research. The human face image was taken from the Human Faces   dataset 14 published under CC0 license.

Furthermore, we tested whether the performance of the classifier utilized as EEG encoder would affect the reconstruction results. We found a correlation of r = 0.48 , p &lt; 0.001 between the ability of the classifier to correctly classify an image and the reconstruction performance for that image as measured by the 1000 trial 50-class top-1 accuracy on the best-generated sample. The reconstruction based on the inter-trial signals yielded a 1000 trial 50-class top-1 accuracy of 4.5% on the best-generated images and an average 1000 trial 50-class top-1 accuracy across the five generated samples per image of 3.2%.

Discussion

The goal of our study was to investigate whether we could classify the category of a perceived image from its VEP and reconstruct the image, utilizing a portable, low-density EEG. Our findings, highlighted by the standout performance of the EEGNet model, demonstrate the feasibility of employing a low-density EEG system for discerning image classes from VEPs. Additionally, the successful generation of images from the base test set displays

Vol.:(0123456789)

Vol:.(1234567890)

Figure 7. Selection of reconstructions for an example of each image class from the advanced test set. Ground truth is presented on the left with the reconstructed image to the right for each column, respectively. The presented ground truth images were taken from the ImageNet Large Scale Visual Recognition   Challenge 13 dataset which permits usage for non-commercial research.

the capability of our simplified setup to reconstruct images from image classes that have been used during finetuning. Notably, for both classification and reconstruction, the outcomes were the best for image categories with facial features or those depicting animals. However, the reconstruction efforts were confined to image classes encountered during training, as evidenced by the drastically lower 1000 trial 50-class top-1 accuracies. Thus, while our setup shows promise for visual decoding of certain image classes, its effectiveness is limited if unseen classes are introduced during testing. It remains to be established whether this limitation is due to the use of a low-density EEG system, or pertains to other decisions taken in this study, as discussed below.

Regarding the key challenges, we believe to have greatly improved the cost and flexibility. Besides the affordable EEG hardware, the best classification model only contained about 62.000 parameters and even the reconstruction task could be fine-tuned on a single NVIDIA GeForce RTX 3070 Laptop GPU. Both the cost and efficiency increase the feasibility of replicating and extending our study by other labs and researchers. Additionally, we provided a new dataset that avoids the data acquisition mistakes made by prior   studies  and ascertained 8 that the data contains stimulus-associated VEPs for which both the   timing 934 , as well as the typical N1-P2 onset esponse r 43 were in line with previous research. Similarly, the reduced accuracy for dry recordings was congruent with prior   findings 44 .

A direct comparison with previous EEG-based VEP classification studies remains challenging. The most widely used EEG-image   dataset  has recorded the neural responses of 6 subjects observing 40 naturalistic image 7 classes containing 50 samples per class with a 128-channel EEG. However, this dataset has given rise to several studies reporting extremely high accuracies of up to 99.5% 45 . As has been pointed out by several   researchers 5,8 , these performances are a result of the utilized block design which enables to predict the image class not from the stimulus-evoked activity, but from the time-related changes in the measurement tool. Notably, this dataset has been employed in at least 10   studies  and its use   continues 5 46 . Thus, while our classification results were clearly above chance level prediction (5%), we struggle to put the performance in perspective with regards to absolute numbers. However, in relative terms, the studies that have utilized the EEG-ChannelNet 26 and the EEGto-Image   paradigm 28 on the aforementioned dataset have reported classification accuracies of 48% and 64%, respectively, which outperformed the EEGNet (32%). Interestingly, this pattern was reversed in our research with the EEGNet clearly outperforming the other two models and the EEG-to-Image approach ranging around chance prediction. While this could be partly due to the adaptations we made to the models, the phenomenon might be better explained by the artifactual predictions based on the block-level temporal   correlations . Namely, 8 we had to make similar modifications with the EEGNet and explored several different model configurations for the EEG-ChannelNet and EEG-to-Image paradigm in the hyperparameter search. A more detailed discussion of why we think the EEG-to-Image framework was inferior can be found in the Supplementary Materials. To our knowledge our study was the first to use the TSCeption and the EEG Conformer model for visual decoding. Remarkably, both models exhibited similar classification performances despite their different architectures and outperformed the EEG-ChannelNet and EEG-to-Image framework. In general, one should note that the EEGNet model was a well-established CNN that has been tested in multiple EEG-based classification tasks,

among which were several visual decoding   studies 3,4 . In contrast, the other models have only been evaluated on the same flawed dataset (EEG-ChannelNet and EEG-to-Image) or were never used for visual decoding. Consequently, achieving optimal performance with these models might require more elaborate tweaking compared to the already optimized EEGNet.

As can be seen in Table 1, there was a high variability of classification performances between subjects. Unfortunately, inter-subjective variability is a common phenomenon in EEG   studies 47 , but also in projects using other neuro-recording methods, like   fMRI . The potential reasons for this variability are multifaceted, including 1 differences in age, sex, attention span, and anatomy, to name a   few 48,49 . A different attempt to explain the interindividual differences may be to look at the signal quality, as indicated by the relative number of bad trials that had to be dropped for a participant's recordings. For this, we have plotted the classification accuracy against the percentage of dropped data and the total number of trials per person, respectively, in Supplementary Fig. 5. The Spearman rank-order correlation between the accuracy and percentage of dropped data and between the accuracy and the total number of trials were non-significant with r = -0.24 , p = 0.57 and r = 0.05 , p = 0.91 , respectively. Therefore, we do not see a connection between the signal quality, as indicated by the percentage of dropped data, and the classification performance in this study. However, these results should be treated with caution as the small sample size limits the potential for generalizations. Lastly, the subject's motivation and attention during the experiment may play a vital role in the obtained results. While we tried to ensure optimal attention during recordings by minimizing distractions and offering incentives with the bonus compensation, we were not able to further account for this variable.

Regarding the classification accuracy per image class, it was not surprising that the Face category obtained the best results. As pointed out before, we specifically selected the Face image class as a stimulus category in this study as it was known to evoke distinct neural   responses 9,15 . Surprisingly, across subjects, the Red Wine and Airliner categories had the best accuracy after the Face class. Previous studies have shown distinct brain responses to observing animate objects, compared to inanimate   ones 50 . Furthermore, the property of looking like an animal has been shown to significantly explain unique variance in the EEG response to animate   objects 51 . In theory, this would make the detection of the animal classes in our dataset easier, compared to images classes like Red Wine . In fact, this pattern was observed for subject 1 for which the Tiger and Panda classes performed better than most inanimate categories. Arguably, the Dog class was an exception, yet, a look at the confusion matrix clarifies that the majority of its misclassifications belonged to one of the other two animal categories. However, across participants, this effect was not observable. While the Tiger class obtained a high accuracy of 45.42%, the other animal classes ranged below 30% accuracy. A potential reason for the success rate of the Red Wine and Airliner categories might be the distinct background that these image classes had, as displayed in Supplementary Fig. 1. However, this assumption would require further testing. With regards to the Pretzel class which was hardest to classify, one should note the relatively high inter-class variability due to the different orientations of the Pretzel images. Possibly for this reason, the EEGNet model struggled to classify the associated VEPs correctly.

Concerning the reconstruction part of this study, we first evaluated the fine-tuned LDM on the base test set. As expected, the model worked best on images with facial features or animal classes, analogous to the classification task. However, the efficiency of the pretrained EEG encoder also limited the reconstruction which can be observed when one image class was mistaken as another one, like the generation of a tiger for the ground truth image of a dog in Fig. 6B. Additionally, we observed a positive moderate correlation between the classifier's proficiency in accurately classifying an image and the 1000 trial 50-class top-1 accuracy calculated on the best reconstruction of the same image. This indicated that the reconstructions were superior for the images which the classifier predicted correctly. However, the embeddings did not only contain the predicted class information. The encoder clearly utilized other high-level information, like the shape and location of an object in the respective image. This becomes evident when inspecting the misclassifications in Figure 6B, where the rough shape and location of the ground truth were frequently preserved. In contrast, the color of the reconstructions was often wrong. A potential reason for this could be that the color of a perceived object seems to be encoded in the frontal rather than in occipito-parietal regions of the   brain 52 . In turn, studies have shown that the object's shape and texture are processed in   V4 5354 , a part of the visual cortex that was located in the vicinity of our electrode locations. Notably, the mean 1000 trial 50-class top-1 accuracy across the five samples per image showed that the image reconstruction was consistent over sampling trials. Additionally, we showed that the reconstruction performance was not solely driven by the EEG encoder, but depended on the EEG input as indicated by the drastically lower accuracies for the inter-trial signals that were not associated with a presented stimulus.

To compare our approach to prior studies, we collected another test dataset with 10 previously unseen image classes. Compared to the 1000 trial 50-class top-1 accuracy of 27.4% achieved   by  and that of other fMRI-based 1 reconstruction   studies 55 , we were clearly below with 8.2%. Thus, our method was able to reconstruct images of classes observed during training, especially, images with facial features or the ones depicting animals. However, in contrast to the fMRI-based paradigms, we were not able to consistently reconstruct unseen image classes with our EEG setup. While there could be multiple reasons for that, the most obvious is the drastically lower spatial resolution of our EEG approach. While the temporal information of VEPs can be used to distinguish perceived image   categories 18,56 , object processing is distributed across several brain   areas 57,58 . Namely, there is a spatially discernable processing hierarchy in which early areas, like V1, encode low-level features, whereas higher layers of the visual cortex and structures in the inferotemporal cortex are responsible for the detection of complex shapes and object   recognition . Therefore, the poor spatial resolution of EEG and its limited ability to record from deeper 5 brain structures inevitably misses additional information to correctly classify the perceived images. Moreover, the self-supervised paradigm employed in prior   studies 1,55 to pretrain the encoder might capture more general patterns that allow for better generalization to unseen image classes.

However, one should also take into consideration that most prior research involved costly equipment (fMRI) and would practically be impossible to work in real-world scenarios. Firstly, because of the bulky, stationary

Vol.:(0123456789)

Vol:.(1234567890)

nature of the device, and secondly because of the hemodynamic response. With regards to the latter, one would need to wait multiple seconds before even observing the evoked activity in the fMRI signal. Additionally, the selfsupervised techniques, like in Chen et al.  employed much larger datasets (136000 fMRI segments) to pretrain 1 the encoder on 8 RTX3090ti GPUs. While our approach is closer to practical applications due to its flexibility, affordability, and simplicity, there remain challenges to improving the proposed setup before transitioning it from a laboratory setting.

First, while we employed a portable and easy to setup device, we still used an artificial lab environment to minimize distractions. Future studies should explore the tradeoff between the increased flexibility and the reduction in the SNR, possibly allowing subjects greater freedom of movement during image presentation. Second, while improving on the temporal constraints of fMRI approaches, the real-time usage would still be limited for two reasons. Firstly, we employed a non-causal zero-phase filter in our preprocessing pipeline to preserve the signal's temporal characteristics requiring future data to perform the filtering. Therefore, future studies should consider a causal filter when attempting the approach in real time. Secondly, the iterative inference process of the LDM required a significant computing time. Reducing the number of sampling steps might lessen the reconstruction quality and would still fall short of achieving real-time capability. However, there have been recent advances trying to enable real-time image generation using Adversarial Diffusion   Distillation 59 , which could potentially address the aforementioned challenge. Third, our study solely focused on the feasibility of identifying and generating perceived images. However, for practical applications, the classification and reconstruction of 'imagined' (mentally visualized) images might be more relevant. A logical next step of our approach would be to test the pipeline with imagined rather than perceived images. As for this study, the image presentation and associated imagination should happen in a random order to avoid artifactual predictions. Fourth, our reconstruction performance was limited by the ability of the pretrained encoder to extract useful information to condition the LDM. While one possibility would be to look for superior classification models, this might not solve the problem. As has been shown in the fMRI-based reconstruction   studies 155 , an encoder that has been pre-trained in a self-supervised paradigm might pick up patterns that generalize better to brain signals evoked by unseen classes. Previously, Bai et al. 46 employed masked signal modeling to pretrain an EEG encoder for reconstruction, however, the EEG-image data has been taken from the flawed block-design   dataset 60 . Recently there have been further methods to create pretrained EEG models that could function as the   encoder 61 .

While there remains a long road towards creating a fully practical end-to-end system that could reconstruct images from someone's perception or visual imagination, we believe to have contributed a valuable step to bring such a system closer to the real world.

Data availibility

The code of this study is openly available at https://  github.  com/  mitme  dialab/  eegre  const  ructi  on. The data will be made available to everyone upon request via this form https://  forms.  gle/  hXxuy  GStUV  gWrQb  w9.

Received: 19 January 2024; Accepted: 28 June 2024

References

- 1.  Chen, Z., Qing, J., Xiang, T., Yue, W . L. &amp; Zhou, J. H. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 22710-22720 (2022).
- 2.  Benchetrit, Y., Banville, H. &amp; King, J.-R. Brain decoding: Toward real-time reconstruction of visual perception, https://  doi.  org/  10. 48550/  arXiv.  2310.  19812 (2023). arXiv: 2310.  19812.
- 3.  Lee, S., Jang, S. &amp; Jun, S. C. Exploring the ability to classify visual perception and visual imagery eeg data: Toward an intuitive bci system. Electronics 11 , 2706 (2022).
- 4.  Shimizu, H. &amp; Srinivasan, R. Improving classification and reconstruction of imagined images from eeg signals. PLoS ONE 17 , 1-16. https://  doi.  org/  10.  1371/  journ  al.  pone.  02748  47 (2022).
- 5.  Holly Wilson, M. G. M. J. P ., Xi Chen &amp; O'Neill, E. Feasibility of decoding visual information from eeg. Brain-computer interfaces, 1-28, https://  doi.  org/  10.  1080/  23262  63X.  2023.  22877  19 (2023).
- 6.  Van Den Boom, M. A., Vansteensel, M. J., Koppeschaar, M. I., Raemaekers, M. A. H. &amp; Ramsey, N. F. Towards an intuitive communication-BCI: Decoding visually imagined characters from the early visual cortex using high-field fMRI. Biomed. Phys. Eng. Express 5 , 055001. https://  doi.  org/  10.  1088/  2057-  1976/  ab302c (2019).
- 7.  Spampinato, C. et al. Deep learning human mind for automated visual classification. In In 2017 IEEE conference on computer vision and pattern recognition (CVPR), 4503-4511, https://  doi.  org/  10.  1109/  CVPR.  2017.  479 (2017).
- 8.  Li, R. et al. The perils and pitfalls of block design for eeg classification experiments. IEEE Trans. Pattern Anal. Mach. Intell. 43 , 316-333. https://  doi.  org/  10.  1109/  TPAMI.  2020.  29731  53 (2021).
- 9.  Kaneshiro, B., Perreau Guimaraes, M., Kim, H.-S., Norcia, A. M. &amp; Suppes, P . A representational similarity analysis of the dynamics of object processing using single-trial eeg classification. PLOS ONE 10 , 1-27 (2015).
- 10. Simanova, I., van Gerven, M., Oostenveld, R. &amp; Hagoort, P . Identifying object categories from event-related eeg: Toward decoding of conceptual representations. PLoS ONE 5 , 1-12. https://  doi.  org/  10.  1371/  journ  al.  pone.  00144  65 (2011).
- 11. Klem, G. H., Lüders, H., Jasper, H. H. &amp; Elger, C. E. The ten-twenty electrode system of the international federation the international federation of clinical neurophysiology. Electroencephal. Clin. Neurophysiol. 52 , 3-6 (1999).
- 12. Kothe, C. Lab streaming layer (lsl) - a software framework for synchronizing a large array of data collection and stimulation devices. Computer software (2014).
- 13. Russakovsky, O. et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vision 115 , 211-252. https://  doi.  org/  10. 1007/  s11263-  015-  0816-y (2015).
- 14. Gupta, A. Human faces [dataset]. Kaggle (2021 (Accessed January 10, 2024)). https://  www.  kaggle.  com/  datas  ets/  ashwi  ngupt  a3012/ human-  faces.
- 15. Nichols, D., Betts, L. &amp; Wilson, H. Decoding of faces and face components in face-sensitive human visual cortex. Front. Psychol. 1 , 1367. https://  doi.  org/  10.  3389/  fpsyg.  2010.  00028 (2010).

- 16. Contini, E. W., Wardle, S. G. &amp; Carlson, T. A. Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. Neuropsychologia 105 , 165-176. https://  doi.  org/  10.  1016/j.  neuro  psych  ologia.  2017.  02.  0110.  1016/j. neuro  psych  ologia.  2017.  02.  0110.  1016/j.  neuro  psych  ologia.  2017.  02.  013 (2017).
- 17. Teichmann, L. et al. The influence of object-color knowledge on emerging object representations in the brain. J. Neurosci. 40 , 6779-6789. https://  doi.  org/  10.  1523/  JNEUR  OSCI.  0158-  20.  2020 (2020).
- 18. Carlson, T., Tovar, D. A., Alink, A. &amp; Kriegeskorte, N. Representational dynamics of object vision: The first 1000 ms. J. Vision 13 , 1-1. https://  doi.  org/  10.  1167/  13.  10.1 (2013).
- 19. Grootswagers, T., Zhou, I., Robinson, A. K., Hebart, M. N. &amp; Carlson, T. A. Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams. Sci. Data 9 , 3. https://  doi.  org/  10.  1038/  s41597-  021-  01102-7 (2022).
- 20. Lee, S., Jang, S. &amp; Jun, S. C. Exploring the ability to classify visual perception and visual imagery eeg data: Toward an intuitive bci system. Electronics 11 , 2706. https://  doi.  org/  10.  3390/  elect  ronic  s1117  2706 (2022).
- 21. Peirce, J. W. et al. Psychopy2: Experiments in behavior made easy. Behav. Res. Methods 51 , 195-203. https://  doi.  org/  10.  3758/ s13428-  018-  01193-y (2019).
- 22. Bigdely-Shamlo, N., Mullen, T., Kothe, C., Su, K.-M. &amp; Robbins, K. A. The prep pipeline: Standardized preprocessing for large-scale eeg analysis. Front. Neuroinform. 9 , 16. https://  doi.  org/  10.  3389/  fninf.  2015.  00016 (2015).
- 23. van Driel, J., Olivers, C. N. &amp; Fahrenfort, J. J. High-pass filtering artifacts in multivariate classification of neural time series data. J. Neurosci. Methods 352 , 109080. https://  doi.  org/  10.  1016/j.  jneum  eth.  2021.  109080 (2021).
- 24. Lawhern, V. J. et al. EEGNet: A compact convolutional neural network for EEG-based brain-computer interfaces. J. Neural Eng. 15 , 056013. https://  doi.  org/  10.  1088/  1741-  2552/  aace8c (2018).
- 25. Ding, Y. et al. TSception: A deep learning framework for emotion detection using EEG. In 2020 international joint conference on neural networks (IJCNN), 1-7, https://  doi.  org/  10.  1109/  IJCNN  48605.  2020.  92067  50 (2020).
- 26. Palazzo, S. et al. Decoding brain representations by multimodal learning of neural activity and visual features. IEEE Trans. Pattern Anal. Mach. Intell. 43 , 3833-3849. https://  doi.  org/  10.  1109/  TPAMI.  2020.  29959  09 (2021).
- 27. Song, Y., Zheng, Q., Liu, B. &amp; Gao, X. EEG conformer: Convolutional transformer for EEG decoding and visualization. IEEE Trans. Neural Syst. Rehabil. Eng. 31 , 710-719. https://  doi.  org/  10.  1109/  TNSRE.  2022.  32302  50 (2023).
- 28. Mishra, A., Raj, N. &amp; Bajwa, G. Eeg-based image feature extraction for visual classification using deep learning (2022). arXiv: 2209. 13090.
- 29. Chollet, F. Xception: Deep learning with depthwise separable convolutions (2017). arXiv: 1610.  02357.
- 30. Szegedy, C. et al. Going deeper with convolutions. In 2015 IEEE conference on computer vision and pattern recognition (CVPR), 1-9, https://  doi.  org/  10.  1109/  CVPR.  2015.  72985  94 (2015).
- 31. Schirrmeister, R. T. et al. Deep learning with convolutional neural networks for eeg decoding and visualization. Human Brain Map. 38 , 5391-5420. https://  doi.  org/  10.  1002/  hbm.  23730 (2017).
- 32. Zhang, H., Silva, F. H. S., Ohata, E. F., Medeiros, A. G. &amp; Rebouças Filho, P . P . Bi-dimensional approach based on transfer learning for alcoholism pre-disposition classification via eeg signals. Front. Human Neurosci. 14 , 365. https://  doi.  org/  10.  3389/  fnhum.  2020. 00365 (2020).
- 33. Kingma, D. P. &amp; Ba, J A method for stochastic optimization, Adam, 2017), arXiv: 1412.  6980.
- 34. Ng, A. Y. Feature selection, l1 vs. l2 regularization, and rotational invariance. In proceedings of the twenty-first international conference on machine learning, ICML '04, 78, https://  doi.  org/  10.  1145/  10153  30.  10154  35 (Association for computing machinery, New York, NY, USA, 2004).
- 35. Smith, L. N. &amp; Topin, N. Super-convergence: Very fast training of neural networks using large learning rates (2018). arXiv: 1708. 07120.
- 36. Rombach, R., Blattmann, A., Lorenz, D., Esser, P . &amp; Ommer, B. High-resolution image synthesis with latent diffusion models. In 2022 IEEE/CVF conference on computer vision and pattern recognition (CVPR)
- 37. Ho, J., Jain, A. &amp; Abbeel, P . Denoising diffusion probabilistic models. In Proceedings of the 34th international conference on neural information processing systems
- 38. Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention - MICCAI 2015 (eds Navab, N. et al. ) 234-241 (Springer International Publishing, Cham, 2015).
- 39. Esser, P ., Rombach, R. &amp; Ommer, B. Taming transformers for high-resolution image synthesis. In 2021 IEEE/CVF conference on computer vision and pattern recognition (CVPR), 12868-12878 (IEEE, New York, 2021).
- 40. Dhariwal, P. &amp; Nichol, A. Diffusion Models Beat GANs on Image Synthesis. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P. S. &amp; Vaughan, J. W . (eds.) Advances in Neural Information Processing Systems, 8780-8794 (Curran Associates, Inc., 2021).
- 41. Liu, L., Ren, Y., Lin, Z. &amp; Zhao, Z. Pseudo numerical methods for diffusion models on manifolds (2022). arXiv: 2202.  09778.

42.

Dosovitskiy, A.

et al.

An image is worth 16x16 words: Transformers for image recognition at scale (2021). arXiv: 2010.  11929.

- 43. Ahmed, H., Wilbur, R. B., Bharadwaj, H. M. &amp; Siskind, J. M. Object classification from randomized eeg trials. In 2021 IEEE/cvf conference on computer vision and pattern recognition (CVPR), 3844-3853, https://  doi.  org/  10.  1109/  CVPR4  6437.  2021.  00384 (2021).
- 44. Pontifex, M. B. &amp; Coffman, C. A. Validation of the gtec unicorn hybrid black wireless EEG system. Psychophysiology 60 , e14320. https://  doi.  org/  10.  1111/  psyp.  14320 (2023).
- 45. Zheng, X. &amp; Chen, W. An attention-based bi-lstm method for visual object classification via eeg. Biomed. Signal Process. Control 63 , 102174. https://  doi.  org/  10.  1016/j.  bspc.  2020.  102174 (2021).
- 46. Bai, Y. et al. Dreamdiffusion: Generating high-quality images from brain eeg signals (2023). arXiv: 2306.  16934.
- 47. Huang, G. et al. Discrepancy between inter- and intra-subject variability in eeg-based motor imagery brain-computer interface: Evidence from multiple perspectives. Front. Neurosci. 17 , 1122661. https://  doi.  org/  10.  3389/  fnins.  2023.  11226  61 (2023).
- 48. Petroni, A. et al. The variability of neural responses to naturalistic videos change with age and sex. eNeuro 5 , 17. https://  doi.  org/ 10.  1523/  ENEURO.  0244-  17.  2017 (2018).
- 49. Smit, D. J. A., Boomsma, D. I., Schnack, H. G., Hulshoff Pol, H. E. &amp; de Geus, E. J. C. Individual differences in eeg spectral power reflect genetic variance in gray and white matter volumes. Twin Res. Human Genet. 15 , 384-392 (2012).
- 50. Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60 , 1126-1141. https://  doi.  org/  10.  1016/j.  neuron.  2008.  10.  043 (2008).
- 51. Jozwik, K. M. et al. Disentangling five dimensions of animacy in human brain and behaviour. Nat. Commun. Biol. 5 , 1247. https:// doi.  org/  10.  1038/  s42003-  022-  04194-y (2022).
- 52. Bird, C. M., Berens, S. C., Horner, A. J. &amp; Franklin, A. Categorical encoding of color in the brain. Proc. Natl. Acad. Sci. 111 , 4590-4595. https://  doi.  org/  10.  1073/  pnas.  13152  75111 (2014).
- 53. Pasupathy, A., Kim, T. &amp; Popovkina, D. V. Object shape and surface properties are jointly encoded in mid-level ventral visual cortex. Curr. Opin. Neurobiol. 58 , 199-208. https://  doi.  org/  10.  1016/j.  conb.  2019.  09.  009 (2019).
- 54. Roe, A. W. et al. Toward a unified theory of visual area v4. Neuron 74 , 12-29. https://  doi.  org/  10.  1016/j.  neuron.  2012.  03.  011 (2012).
- 55. Ozcelik, F., Choksi, B., Mozafari, M., Reddy, L. &amp; VanRullen, R. Reconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans. In 2022 international joint conference on neural networks (IJCNN), 1-8, https://  doi.  org/  10.  1109/  IJCNN  55064.  2022.  98926  73 (2022).

Vol.:(0123456789)

- 56. Teichmann, L. et al. The influence of object-color knowledge on emerging object representations in the brain. J. Neurosci. 40 , 6779-6789. https://  doi.  org/  10.  1523/  JNEUR  OSCI.  0158-  20.  2020 (2020).
- 57. Contini, E. W., Wardle, S. G. &amp; Carlson, T. A. Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. Neuropsychologia 105 , 165-176. https://  doi.  org/  10.  1016/j.  neuro  psych  ologia.  2017.  02.  013 (2017).
- 58. Malach, R., Levy, I. &amp; Hasson, U. The topography of high-order human object areas. Trends Cogn. Sci. 6 , 176-184. https://  doi.  org/ 10.  1016/  s1364-  6613(02)  01870-3 (2002).
- 59. Sauer, A., Lorenz, D., Blattmann, A. &amp; Rombach, R. Adversarial diffusion distillation (2023). arXiv: 2311.  17042.
- 60. Kavasidis, I., Palazzo, S., Spampinato, C., Giordano, D. &amp; Shah, M. Brain2image: Converting brain signals into images. In proceedings of the 25th ACM international conference on multimedia, MM '17, 1809-1817, https://  doi.  org/  10.  1145/  31232  66.  31279 07 (Association for computing machinery, New York, NY, USA, 2017).
- 61. Cui, W. et al. Neuro-gpt: Developing a foundation model for eeg (2023). arXiv: 2311.  03764.

Author contributions

S.G. conducted the experiments, implemented the analyses and interpreted the results under the supervision by N.K.. Additionally, S.G. is responsible for the code accompanying the study. All authors reviewed the manuscript.

Funding

Open Access funding enabled and organized by Projekt DEAL.

Competing interests

The authors declare no competing interests.

Additional information

Supplementary Information The online version contains supplementary material available at https://  doi.  org/ 10.  1038/  s41598-  024-  66228-1.

Correspondence and requests for materials should be addressed to S.G.

Reprints and permissions information is available at www.nature.com/reprints.

Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Open Access This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://  creat  iveco  mmons.  org/  licen  ses/  by/4.  0/.

© The Author(s) 2024

Scientific Reports

|        (2024) 14:16436  |

Vol:.(1234567890)